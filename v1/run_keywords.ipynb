{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ec6715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import boto3\n",
    "import tqdm\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from keybert import KeyBERT\n",
    "import spacy\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer, KeyphraseTfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98bc6dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c04feb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.require_gpu()\n",
    "_ = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f840b934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_keywords(keywords, paper_title):\n",
    "    temp_keywords = []\n",
    "    paper_title = str(paper_title)\n",
    "    \n",
    "    # If all uppercase title, switch to title case\n",
    "    if paper_title.isupper():\n",
    "        paper_title = paper_title.title()\n",
    "    \n",
    "    # Remove unnecessary keywords\n",
    "    for keyword in keywords:\n",
    "        # remove \"et al\" keywords\n",
    "        lower_title = paper_title.lower()\n",
    "        if re.search('\\\\bet al\\\\b', keyword[0]):\n",
    "            pass\n",
    "        elif (f'{keyword[0]}-' in lower_title) or (f'-{keyword[0]}' in lower_title):\n",
    "            pass\n",
    "        else:\n",
    "            temp_keywords.append(keyword)\n",
    "    \n",
    "    paper_title_words = paper_title.split(\" \")\n",
    "            \n",
    "    # Get final version of keywords\n",
    "    final_keywords = []\n",
    "    for keyword in temp_keywords:\n",
    "        if keyword[1] < 0.25:\n",
    "            pass\n",
    "        else:\n",
    "            final_keyword = []\n",
    "            if len(keyword[0].split(\" \")) > 4:\n",
    "                pass\n",
    "            else:\n",
    "                for word in keyword[0].split(\" \"):\n",
    "                    for title_word in paper_title_words:\n",
    "                        if word in title_word.lower():\n",
    "                            final_keyword.append(title_word.replace(\"'s\", \"\").replace(\":\", \"\")\n",
    "                                                 .replace(\")\",\"\").replace(\"(\", \"\")\n",
    "                                                 .replace(\"?\",\"\").replace(\"].\",\"\").replace(\"]\", \"\")\n",
    "                                                 .replace(\"[\", \"\").replace(\".\", \"\").replace(\",\",\"\")\n",
    "                                                 .replace('\"',\"\").replace(\"'\", \"\").replace(\"’\", \"\")\n",
    "                                                 .replace(\"“\",\"\"))\n",
    "                            break\n",
    "                if final_keyword:\n",
    "                    final_keywords.append([\" \".join(final_keyword), str(keyword[1])])\n",
    "                else:\n",
    "                    pass\n",
    "                    \n",
    "    # Hyphenated words as a keyword\n",
    "    if '-' in paper_title:\n",
    "        if re.search('[A-z]+-[A-z]+-*[A-z]+-*[A-z]+', paper_title):\n",
    "            matches = re.findall('[A-z]+-[A-z]+-*[A-z]+-*[A-z]+', paper_title)\n",
    "            for match in matches:\n",
    "                final_keywords.append([match, str(0.25)])\n",
    "            \n",
    "    # Getting rid of single words that are already part of another keyword\n",
    "    all_single_keywords = [x[0] for x in final_keywords if len(x[0].split(\" \"))==1]\n",
    "    single_keywords_to_remove = []\n",
    "    if all_single_keywords:\n",
    "        for keyword in final_keywords:\n",
    "            _ = [single_keywords_to_remove.append(single_keyword) for single_keyword in all_single_keywords \n",
    "                 if ((single_keyword in keyword[0]) and (single_keyword != keyword[0]))]\n",
    "    \n",
    "    if single_keywords_to_remove:\n",
    "        final_keywords = {x[0]:x[1] for x in final_keywords if x[0] not in single_keywords_to_remove}\n",
    "        return [[x,str(y)] for x,y in final_keywords.items()]\n",
    "    else:\n",
    "        return final_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaf24895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_data(docs):\n",
    "    model_preds = kw_model.extract_keywords(docs=docs, vectorizer=KeyphraseCountVectorizer())\n",
    "    \n",
    "    final_keywords = [edit_keywords(x, y) for x,y in zip(model_preds, docs)]\n",
    "    return final_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4d7759a-2b25-4ca0-8320-3f37f9da33f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(lst, n):\n",
    "    # Yield successive n-sized chunks from lst\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27a31a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_keywords(filename, round_name):\n",
    "    titles_to_keyword_df = pd.read_parquet(f\"{filename}\", columns=['paper_title'])\n",
    "    \n",
    "    titles_to_keyword = titles_to_keyword_df['paper_title'].tolist()\n",
    "    \n",
    "    all_keywords = [score_data(chunk) for chunk in \n",
    "                split_list(titles_to_keyword, 10000)]\n",
    "    \n",
    "    file_id = filename.split(\"/part-\")[1].split(\"-\")[0]\n",
    "    \n",
    "    all_keywords_list = [x for y in all_keywords for x in y]\n",
    "    \n",
    "    if len(all_keywords_list) == len(titles_to_keyword):\n",
    "        pd.DataFrame(zip(titles_to_keyword, all_keywords_list), columns=['paper_title','keywords']) \\\n",
    "            .to_parquet(\"keywords.parquet\")\n",
    "    else:\n",
    "        print(\"####### Lengths do not match #######\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3133710-0280-4624-b920-de1b98e5dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init KeyBERT\n",
    "kw_model = KeyBERT(model='all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ccf915",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for file_name in all_work_files:\n",
    "    dt_now = datetime.now(tz=pytz.timezone('US/Eastern')).strftime(\"%H:%M\")\n",
    "    print(dt_now, \" \", file_name)\n",
    "    _ = get_all_keywords(file_name, round_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f828919d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
